"""
Modal Cloud Indexer - Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª MySQL Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
ÙŠØ³ØªØ®Ø¯Ù… GPU Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø© ÙˆÙŠØ¯Ø¹Ù… OCR
"""
import modal
import os
import time
import shutil
from dotenv import load_dotenv

load_dotenv()

# 1. ØªØ¹Ø±ÙŠÙ Ø§Ù„ØµÙˆØ±Ø© (Ù…Ø¹ Ø¥Ø¶Ø§ÙØ© MySQL Ùˆ Tesseract OCR)
image = (
    modal.Image.from_registry("nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04", add_python="3.10")
    .apt_install(
        "git", 
        "ffmpeg", 
        "libgl1-mesa-glx", 
        "tesseract-ocr", 
        "tesseract-ocr-ara", 
        "tesseract-ocr-eng",
        "default-libmysqlclient-dev",  # Ù…ÙƒØªØ¨Ø§Øª MySQL
        "pkg-config"
    )
    .pip_install(
        "langchain",
        "langchain-community",
        "langchain-huggingface",
        "langchain-openai",
        "faiss-cpu",
        "pypdf",
        "pymupdf",
        "sentence-transformers",
        "python-dotenv",
        "python-pptx",
        "docx2txt",
        "Pillow",
        "pytesseract",
        "mysql-connector-python",  # Ù„Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ MySQL
        "huggingface_hub"
    )
    .add_local_file("config.py", remote_path="/root/smart_homework_helper/config.py")
    .add_local_dir("filters", remote_path="/root/smart_homework_helper/filters")
    .add_local_dir("pipelines", remote_path="/root/smart_homework_helper/pipelines")
)

app = modal.App("homework-helper-db-indexer")

@app.function(
    image=image,
    gpu="T4",
    timeout=3600,
    secrets=[
        modal.Secret.from_dict({
            "HUGGINGFACE_API_KEY": os.getenv("HUGGINGFACE_API_KEY", ""),
            "OPENROUTER_API_KEY": os.getenv("OPENROUTER_API_KEY", ""),
            "HF_TOKEN": os.getenv("HUGGINGFACE_API_KEY", ""),
            # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            "MYSQL_HOST": os.getenv("MYSQL_HOST", "localhost"),
            "MYSQL_USER": os.getenv("MYSQL_USER", "rawad"),
            "MYSQL_PASSWORD": os.getenv("MYSQL_PASSWORD", "1234"),
            "MYSQL_DATABASE": os.getenv("MYSQL_DATABASE", "edu_system"),
            "MYSQL_PORT": os.getenv("MYSQL_PORT", "3306")
        })
    ]
)
def run_cloud_db_indexer():
    """
    ØªØ´ØºÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
    """
    import sys
    sys.path.append("/root/smart_homework_helper")
    
    from config import Config
    
    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ù„Ù„Ø³Ø­Ø§Ø¨Ø©
    Config.DATA_DIR = "/root/smart_homework_helper/data"
    Config.VECTOR_DB_PATH = "/root/smart_homework_helper/faiss_index_cloud"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
    os.makedirs(Config.DATA_DIR, exist_ok=True)
    os.makedirs(Config.VECTOR_DB_PATH, exist_ok=True)
    
    from filters.mysql_loader import MySQLLoader
    from pipelines.text_pipeline import TextPipeline
    from pipelines.image_pipeline import ImagePipeline
    
    print("=" * 70)
    print("ğŸš€ Starting Cloud Database Indexer on Modal GPU")
    print("=" * 70)
    start_time = time.time()
    
    # ========================================
    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† MySQL
    # ========================================
    print("\nğŸ“¥ [Phase 1] Loading Data from MySQL Database...")
    try:
        loader = MySQLLoader()
        documents = loader.load_data()
        
        if not documents:
            print("âš ï¸ No documents found in database!")
            return None
        
        print(f"âœ… Loaded {len(documents)} documents from database")
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        lesson_docs = [d for d in documents if d.metadata.get('type') == 'lesson']
        qa_docs = [d for d in documents if d.metadata.get('type') == 'qa_explanation']
        
        print(f"   ğŸ“š Lesson content: {len(lesson_docs)}")
        print(f"   â“ Q&A explanations: {len(qa_docs)}")
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"âŒ Database Loading Error: {e}")
        return None
    
    # ========================================
    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ù†ØµÙˆØµ
    # ========================================
    print("\nğŸ“ [Phase 2] Building Text Index (with OCR support)...")
    try:
        text_pipeline = TextPipeline(Config)
        text_pipeline.build_index_from_documents(documents)
        print("âœ… Text indexing complete.")
        
        # Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙÙ‡Ø±Ø³
        if text_pipeline.vectorstore:
            total_vectors = text_pipeline.vectorstore.index.ntotal
            print(f"   ğŸ“Š Total vectors in index: {total_vectors}")
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"âŒ Text Pipeline Error: {e}")
        return None
    
    # ========================================
    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ÙÙ‡Ø±Ø³Ø© Ø§Ù„ØµÙˆØ± Ù…Ù† PDFs (Ø¥Ù† ÙˆØ¬Ø¯Øª)
    # ========================================
    print("\nğŸ–¼ï¸ [Phase 3] Image Indexing from PDFs...")
    try:
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª PDF ÙÙŠ data directory
        pdf_files = []
        if os.path.exists(Config.DATA_DIR):
            pdf_files = [
                os.path.join(Config.DATA_DIR, f) 
                for f in os.listdir(Config.DATA_DIR) 
                if f.endswith('.pdf')
            ]
        
        if pdf_files:
            print(f"   ğŸ“„ Found {len(pdf_files)} PDF files")
            image_pipeline = ImagePipeline(Config)
            image_pipeline.run(files_to_process=pdf_files)
            print("âœ… Image indexing complete.")
        else:
            print("   â„¹ï¸ No PDF files found for image extraction")
            print("   ğŸ’¡ Tip: Upload PDFs to data/ folder for image indexing")
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"âš ï¸ Image Pipeline Warning: {e}")
    
    # ========================================
    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙÙ‡Ø±Ø³
    # ========================================
    print("\nğŸ” [Phase 4] Verifying Index...")
    try:
        test_pipeline = TextPipeline(Config)
        test_pipeline.load_index()
        
        if test_pipeline.vectorstore:
            # Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø­Ø« Ø¨Ø³ÙŠØ·
            test_query = "Ù…Ø§ Ù‡Ùˆ"
            results = test_pipeline.vectorstore.similarity_search(test_query, k=3)
            
            print(f"âœ… Index verification successful")
            print(f"   ğŸ“Š Total vectors: {test_pipeline.vectorstore.index.ntotal}")
            print(f"   ğŸ” Test search: Found {len(results)} results")
            
            if results:
                print("\n   ğŸ“ Sample results:")
                for i, doc in enumerate(results[:2], 1):
                    preview = doc.page_content[:80].replace('\n', ' ')
                    print(f"      {i}. {preview}...")
        else:
            print("âš ï¸ Index verification failed")
            return None
            
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"âŒ Verification Error: {e}")
        return None
    
    end_time = time.time()
    print(f"\nâœ… Indexing Complete in {end_time - start_time:.2f} seconds.")
    
    # ========================================
    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ø­Ø²Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ù„ØªÙ†Ø²ÙŠÙ„
    # ========================================
    print("\nğŸ“¦ [Phase 5] Packaging Results...")
    output_package = "/root/output_package"
    if os.path.exists(output_package):
        shutil.rmtree(output_package)
    os.makedirs(output_package)
    
    # Ù†Ø³Ø® Ø§Ù„ÙÙ‡Ø±Ø³
    if os.path.exists(Config.VECTOR_DB_PATH):
        shutil.copytree(
            Config.VECTOR_DB_PATH, 
            os.path.join(output_package, "faiss_index")
        )
        print("   âœ… FAISS index packaged")
    
    # Ù†Ø³Ø® debug texts (Ø¥Ù† ÙˆØ¬Ø¯Øª)
    debug_texts_source = os.path.join(Config.DATA_DIR, "debug_extracted_texts")
    if os.path.exists(debug_texts_source):
        shutil.copytree(
            debug_texts_source, 
            os.path.join(output_package, "debug_texts")
        )
        print("   âœ… Debug texts packaged")
    
    # Ù†Ø³Ø® Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© (Ø¥Ù† ÙˆØ¬Ø¯Øª)
    extracted_images_source = os.path.join(Config.DATA_DIR, "extracted_images")
    if os.path.exists(extracted_images_source):
        shutil.copytree(
            extracted_images_source, 
            os.path.join(output_package, "extracted_images")
        )
        print("   âœ… Extracted images packaged")
    
    # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ØªÙˆÙ‚ÙŠØª (Ù„ØªØ¬Ù†Ø¨ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø¶ØºØ·)
    current_time = time.time()
    for root, dirs, files in os.walk(output_package):
        for f in files:
            try:
                os.utime(os.path.join(root, f), (current_time, current_time))
            except:
                pass
        for d in dirs:
            try:
                os.utime(os.path.join(root, d), (current_time, current_time))
            except:
                pass
    
    # Ø¶ØºØ· Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print("   ğŸ—œï¸ Compressing results...")
    shutil.make_archive("/root/final_output", 'zip', output_package)
    
    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¶ØºÙˆØ·
    with open("/root/final_output.zip", "rb") as f:
        zip_bytes = f.read()
    
    print(f"   ğŸ“¦ Package size: {len(zip_bytes) / (1024*1024):.2f} MB")
    
    return zip_bytes

@app.local_entrypoint()
def main():
    """
    Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„Ù…Ø­Ù„ÙŠØ© - ØªØ´ØºÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³Ø© ÙˆØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    """
    print("=" * 70)
    print("ğŸŒ©ï¸  Modal Cloud Database Indexer")
    print("=" * 70)
    print("\nğŸ“¤ Triggering cloud indexer...")
    print("â³ This may take several minutes depending on database size...\n")
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©
    zip_bytes = run_cloud_db_indexer.remote()
    
    if not zip_bytes:
        print("\nâŒ Indexing failed! Check the logs above.")
        return
    
    print("\nğŸ“¥ Downloading results...")
    with open("cloud_db_results.zip", "wb") as f:
        f.write(zip_bytes)
    
    print("ğŸ“¦ Extracting results...")
    
    # Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
    folders_to_clean = [
        "faiss_index",
        "text_index", 
        os.path.join("data", "debug_extracted_texts"),
        os.path.join("data", "extracted_images")
    ]
    
    for folder in folders_to_clean:
        if os.path.exists(folder):
            try:
                shutil.rmtree(folder)
                print(f"   ğŸ—‘ï¸ Cleaned old {folder}")
            except Exception as e:
                print(f"   âš ï¸ Could not clean {folder}: {e}")
    
    # ÙÙƒ Ø§Ù„Ø¶ØºØ·
    shutil.unpack_archive("cloud_db_results.zip", ".")
    
    # Ù†Ù‚Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØµØ­ÙŠØ­Ø©
    if os.path.exists("faiss_index"):
        print("   âœ… FAISS index extracted")
    
    if os.path.exists("debug_texts"):
        target = os.path.join("data", "debug_extracted_texts")
        os.makedirs(os.path.dirname(target), exist_ok=True)
        if os.path.exists(target):
            shutil.rmtree(target)
        shutil.move("debug_texts", target)
        print(f"   ğŸ“„ Debug texts saved to: {target}")
    
    if os.path.exists("extracted_images"):
        target = os.path.join("data", "extracted_images")
        os.makedirs(os.path.dirname(target), exist_ok=True)
        if os.path.exists(target):
            shutil.rmtree(target)
        shutil.move("extracted_images", target)
        print(f"   ğŸ–¼ï¸ Extracted images saved to: {target}")
    
    print("\n" + "=" * 70)
    print("ğŸ‰ Cloud Database Indexing Complete!")
    print("=" * 70)
    print("\nğŸ“Œ Next Steps:")
    print("   1. Start API: uvicorn api:app --reload")
    print("   2. Test: http://localhost:8000/docs")
    print("   3. Try a question!")
    print("\n")

if __name__ == "__main__":
    # ÙŠÙ…ÙƒÙ† ØªØ´ØºÙŠÙ„Ù‡ Ù…Ø¨Ø§Ø´Ø±Ø©
    print("ğŸ’¡ To run this script, use: modal run modal_db_indexer.py")
